# Configuration and parameters

GPT-NeoX parameters are defined in a YAML configuration file which is passed to the `deepy.py` launcher - for examples see the files contained in this folder.
Parameters originate from either the [DeepSpeed runner CLI (DSL)](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/launcher/runner.py#L33), [DeepSpeed configuration file (DSC)](https://www.deepspeed.ai/docs/config-json/), [Megatron-LM CLI (Meg)](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L224) or are GPT-NeoX (NeoX) modifications.
When the parameters are from either DeepSpeed or Megatron the description has be copied from the respective documentation with modifications.

| Origin | Parameter name                            | Default value | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|--------|-------------------------------------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| DSL    | hostfile                                  | /job/hostfile | Hostfile path (in MPI style) that defines the resource pool available to the job (e.g., `worker-0 slots=4`)                                                                                                                                                                                                                                                                                                                                                                                                                    |
| DSL    | include                                   |               | Specify hardware resources to use during execution. String format is `NODE_SPEC[@NODE_SPEC ...]` where `NODE_SPEC=NAME[:SLOT[,SLOT ...]]`. If `:SLOT` is omitted, include all slots on that host. Example: `"worker-0@worker-1:0,2"` will use all slots. on `worker-0` and slots `[0, 2]` on `worker-1`.                                                                                                                                                                                                                       |
| DSL    | exclude                                   |               | Specify hardware resources to NOT use during execution. Same format as `include`.                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| DSL    | num_nodes                                 | -1            | Total number of worker nodes to run on, this will use the top N hosts from the given hostfile. `-1` will use all.                                                                                                                                                                                                                                                                                                                                                                                                              |
| DSL    | num_gpus                                  | -1            | Max number of GPUs to use on each node, will use [0:N) GPU ids on each node. `-1` will use all.                                                                                                                                                                                                                                                                                                                                                                                                                                |
| DSL    | master_port                               | 29500         | Port used by PyTorch distributed for communication during training.                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| DSL    | master_addr                               |               | IP address of node 0, will be inferred via 'hostname -I' if not specified.                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| DSL    | launcher                                  | pdsh          | Launcher backend for multi-node training. Options currently include PDSH, OpenMPI, MVAPICH.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | num-layers                                |               | Number of transformer layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Meg    | num-unique-layers                         |               | Number of unique transformer layers. `num-layers` should be divisible by this value.                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | param-sharing-style                       | grouped       | Ordering of the shared parameters. For example, for a `num-layers`=4 and `--num-unique-layers`=2, we will have the following ordering for two unique layers 1 and 2-: grouped: [1, 2, 1, 2] and spaced: [1, 1, 2, 2].                                                                                                                                                                                                                                                                                                          |
| Meg    | hidden-size                               |               | Tansformer hidden size.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | num-attention-heads                       |               | Number of transformer attention heads.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Meg    | max-position-embeddings                   |               | Maximum number of position embeddings to use. This is the size of position embedding.                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Meg    | make-vocab-size-divisible-by              | 128           | Pad the vocab size to be divisible by this value. This is added for computational efficiency reasons.                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | layernorm-epsilon                         | 1e-05         | Layer norm epsilon.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | apply-residual-connection-post-layernorm  | false         | If set, use original BERT residula connection ordering.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | openai-gelu                               | false         | Use OpenAIs GeLU implementation. This option should not be used unless for backward compatibility reasons.                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | onnx-safe                                 |               | Use workarounds for known problems with Torch ONNX exporter                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | attention-dropout                         | 0.1           | Post attention dropout probability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | hidden-dropout                            | 0.1           | Dropout probability for hidden state transformer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Meg    | weight-decay                              | 0.01          | Weight decay coefficient for L2 regularization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Meg    | adam-beta1                                | 0.9           | First coefficient for computing running averages of gradient and its square.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Meg    | adam-beta2                                | 0.999         | Second coefficient for computing running averages of gradient and its square.                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Meg    | adam-eps                                  | 1e-08         | Term added to the denominator to improvenumerical stability                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | batch-size                                |               | Batch size per model instance (local batch size). Global batch size is local batch size times data parallel size.                                                                                                                                                                                                                                                                                                                                                                                                              |
| NeoX   | onebitadam                                | false         | Enable one bit adam optimizer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | checkpoint-activations                    | false         | Checkpoint activation to allow for training with larger models, sequences, and batch sizes.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | distribute-checkpointed-activations       | false         | If set, distribute checkpointed activations across model parallel group.                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Meg    | checkpoint-num-layers                     | 1             | Chunk size (number of layers) for checkpointing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | train-iters                               |               | Total number of iterations to train over all training runs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | log-interval                              | 100           | Report loss and timing interval.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | exit-interval                             |               | Exit the program after the iteration is divisible by this value.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | tensorboard-dir                           |               | Write TensorBoard logs to this directory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Meg    | scaled-upper-triang-masked-softmax-fusion | false         | Enable fusion of query_key_value_scaling time (upper diagonal) masking and softmax.                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | scaled-masked-softmax-fusion              | false         | Enable fusion of query_key_value_scaling general masking and softmax.                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Meg    | bias-gelu-fusion                          | false         | Enable bias and gelu fusion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| NeoX   | geglu                                     | false         | Enable geglu activation function (WARNING: will increase memory usage, adjust embd dims accordingly)                                                                                                                                                                                                                                                                                                                                                                                                                           |
| NeoX   | no-weight-tying                           | false         | Disables weight tying between embedding weights and final Linear layer                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| NeoX   | sinusoidal-pos-emb                        | false         | Uses Sinusoidal Positional embedding applied to the inputs instead of learned                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| NeoX   | rpe                                       | false         | T5 relative positional encoding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| NeoX   | rpe-causal                                | false         | T5 relative positional encoding causal flag                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| NeoX   | rpe-num-buckets                           | 32            | T5 relative positional encoding number of buckets, default 32.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| NeoX   | rpe-max-distance                          | 128           | T5 relative positional encoding max distance, default 128.                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | bias-dropout-fusion                       | false         | Enable bias and dropout fusion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| NeoX   | sparsity                                  | none          | Sparse attention layer configuration: `none` = all regular attn, `all` = all sparse attn, `interspersed` = sparse on odd layers, dense on even.                                                                                                                                                                                                                                                                                                                                                                                |
| NeoX   | rms-norm                                  | false         | Use root mean squared norm                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| NeoX   | rms-norm-epsilon                          | 1e-8          | Root mean squared norm epsilon                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | cpu-optimizer                             | false         | Run optimizer on CPU                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | cpu_torch_adam                            | false         | Use Torch Adam as optimizer on CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | seed                                      | 1234          | Random seed used for python, numpy, pytorch, and cuda.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Meg    | init-method-std                           | 0.02          | Standard deviation of the zero mean normal distribution used for weight initialization.                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | lr                                        |               | Initial learning rate. Depending on decay style and initial warmup, the learing rate at each iteration would be different.                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | lr-decay-style                            | linear        | Learning rate decay function.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Meg    | lr-decay-iters                            |               | Number of iterations to decay learning rate over, If None defaults to `--train-iters`                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Meg    | min-lr                                    | 0.0           | Minumum value for learning rate. The schedulerclip values below this threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Meg    | warmup                                    | 0.01          | Percentage of total iterations to warmup on (.01 = 1 percent of all training iters).                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | override-lr-scheduler                     | false         | Reset the values of the scheduler (learning rate,warmup iterations, minimum learning rate, maximum number of iterations, and decay style from input arguments and ignore values from checkpoints. Note that all the above values will be reset.                                                                                                                                                                                                                                                                                 |
| Meg    | use-checkpoint-lr-scheduler               | false         | Use checkpoint to set the values of the scheduler (learning rate, warmup iterations, minimum learning rate, maximum number of iterations, and decay style from checkpoint and ignore input arguments.                                                                                                                                                                                                                                                                                                                          |
| Meg    | save                                      |               | Output directory to save checkpoints to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Meg    | save-interval                             |               | Number of iterations between checkpoint saves.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | no-save-optim                             | false         | Do not save current optimizer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | no-save-rng                               | false         | Do not save current rng state.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | load                                      |               | Directory containing a model checkpoint.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Meg    | no-load-optim                             | false         | Do not load optimizer when loading checkpoint.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | no-load-rng                               | false         | Do not load rng state when loading checkpoint.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | finetune                                  | false         | Load model for finetuning. Do not load optimizer or rng state from checkpoint and set iteration to 0. Assumed when loading a release checkpoint.                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | apply-query-key-layer-scaling             | false         | Scale `Q * K^T` by `1 / layer-number`. If this flag is set, then it will automatically set attention-softmax-in-fp32 to `true`                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | attention-softmax-in-fp32                 | false         | Run attention masking and softmax in fp32.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | fp32-allreduce                            | false         | All-reduce in fp32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Meg    | hysteresis                                | 2             | hysteresis for dynamic loss scaling                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | loss-scale                                |               | Static loss scaling, positive power of 2 values can improve fp16 convergence. If None, dynamic loss scaling is used.                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | loss-scale-window                         | 1000          | Window over which to raise/lower dynamic scale.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Meg    | min-scale                                 | 1             | Minimum loss scale for dynamic loss scale.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | fp16-lm-cross-entropy                     | false         | Move the cross entropy unreduced loss calculation for lm head to fp16.                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Meg    | model-parallel-size                       | 1             | Size of the model parallel.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | pipe-parallel-size                        | 0             | Size of the pipeline parallel. Disable with 0.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | distributed-backend                       | nccl          | Which backend to use for distributed training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | DDP-impl                                  | local         | which DistributedDataParallel implementation to use.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | local_rank                                |               | local rank passed from distributed launcher.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Meg    | lazy-mpu-init                             |               | If set to True, initialize_megatron() skips DDP initialization and returns function to complete it instead. Also turns on `use-cpu-initialization` flag. This is for external DDP manager.                                                                                                                                                                                                                                                                                                                                     |
| Meg    | use-cpu-initialization                    | false         | If set, affine parallel weights initialization uses CPU                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | eval-iters                                | 100           | Number of iterations to run for evaluationvalidation/test for.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | eval-interval                             | 1000          | Interval between running evaluation on validation set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Meg    | data-path                                 |               | Path to combined dataset to split.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Meg    | split                                     | 969, 30, 1    | Comma-separated list of proportions for training, validation, and test split. For example the split `90,5,5` will use 90% of data for training, 5% for validation and 5% for test.                                                                                                                                                                                                                                                                                                                                             |
| Meg    | vocab-file                                |               | Path to the vocab file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | merge-file                                |               | Path to the BPE merge file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | seq-length                                |               | Maximum sequence length to process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | mask-prob                                 | 0.15          | Probability of replacing a token with mask.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | short-seq-prob                            | 0.1           | Probability of producing a short sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Meg    | mmap-warmup                               | false         | Warm up mmap files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | num-workers                               | 2             | Dataloader number of workers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Meg    | tokenizer-type                            |               | What type of tokenizer to use.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Meg    | data-impl                                 | infer         | Implementation of indexed datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | reset-position-ids                        | false         | Reset posistion ids after end-of-document token.                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | reset-attention-mask                      | false         | Reset self attention maske after end-of-document token.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | eod-mask-loss                             | false         | Mask loss for the end of document tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Meg    | adlr-autoresume                           | false         | Enable auto-resume on adlr cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Meg    | adlr-autoresume-interval                  | 1000          | Intervals over which check for auto-resume termination signal                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Meg    | ict-head-size                             |               | Size of block embeddings to be used in ICT and REALM (paper default: 128)                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Meg    | ict-load                                  |               | Directory containing an ICTBertModel checkpoint                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Meg    | bert-load                                 |               | Directory containing an BertModel checkpoint (needed to start ICT and REALM)                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Meg    | titles-data-path                          |               | Path to titles dataset used for ICT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Meg    | query-in-block-prob                       | 0.1           | Probability of keeping query in block for ICT dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Meg    | use-one-sent-docs                         | false         | Whether to use one sentence documents in ICT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Meg    | report-topk-accuracies                    | []            | Which top-k accuracies to report (e.g. '1 5 20')                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | faiss-use-gpu                             | False         | Whether create the FaissMIPSIndex on GPU                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Meg    | block-data-path                           |               | Where to save/load BlockData to/from                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | indexer-batch-size                        | 128           | How large of batches to use when doing indexing jobs                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Meg    | indexer-log-interval                      | 1000          | After how many batches should the indexer report progress                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Meg    | deepspeed-activation-checkpointing        | false         | Uses activation checkpointing from deepspeed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Meg    | partition-activations                     | false         | Partition Activations across GPUs before checkpointing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | contigious-checkpointing                  | false         | Contigious memory checkpointing for activations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Meg    | checkpoint-in-cpu                         | false         | Move the activation checkpoints to CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | synchronize-each-layer                    | false         | does a synchronize at the beginning and end of each checkpointed layer.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Meg    | profile-backward                          | false         | Enables backward pass profiling for checkpointed layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Meg    | deepspeed_mpi                             | false         | Run via MPI, this will attempt to discover the necessary variables to initialize torch distributed from the MPI environment                                                                                                                                                                                                                                                                                                                                                                                                    |
| DSC    | train_batch_size                          |               | The effective training batch size. This is the amount of data samples that leads to one step of model update. `train_batch_size` is aggregated by the batch size that a single GPU processes in one forward/backward pass (a.k.a., `train_step_batch_size`), the gradient accumulation steps (a.k.a., `gradient_accumulation_steps`), and the number of GPUs.                                                                                                                                                                  |
| DSC    | train_micro_batch_size_per_gpu            |               | Batch size to be processed by one GPU in one step (without gradient accumulation). When specified, `gradient_accumulation_steps` is automatically calculated using `train_batch_size` and number of GPUs. Should not be concurrently specified with gradient_accumulation_steps in the configuration JSON.                                                                                                                                                                                                                     |
| DSC    | gradient_accumulation_steps               |               | Number of training steps to accumulate gradients before averaging and applying them. This feature is sometimes useful to improve scalability since it results in less frequent communication of gradients between steps. Another impact of this feature is the ability to train with larger batch sizes per GPU. When specified, `train_step_batch_size` is automatically calculated using `train_batch_size` and number of GPUs. Should not be concurrently specified with `train_step_batch_size` in the configuration JSON. |
| DSC    | optimizer                                 |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)                                                                                                                                                                                                                                                                                                                                                                                                         |
| DSC    | scheduler                                 |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)                                                                                                                                                                                                                                                                                                                                                                                                         |
| DSC    | fp32_allreduce                            | false         | During gradient averaging perform allreduce with 32 bit values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| DSC    | prescale_gradients                        | false         | Scale gradients before doing allreduce.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| DSC    | gradient_predivide_factor                 | 1.0           | Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs                                                                                                                                                                                                                                                                                                                                                                              |
| DSC    | sparse_gradients                          | false         | Enable sparse compression of torch.nn.Embedding gradients.                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| DSC    | fp16                                      |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)                                                                                                                                                                                                                                                                                                                                                                                                        |
| DSC    | amp                                       |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)                                                                                                                                                                                                                                                                                                                                                                               |
| DSC    | gradient_clipping                         | 0             | Enable gradient clipping with provided value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| DSC    | zero_optimization                         |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)                                                                                                                                                                                                                                                                                                                                                                                         |
| DSC    | steps_per_print                           | 10            | Print train loss every N steps                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| DSC    | wall_clock_breakdown                      | false         | Enable timing of the latency of forward/backward/update training phases                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| DSC    | dump_state                                | false         | Print out state information of DeepSpeed object after initialization                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| DSC    | flops_profiler                            |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#flops-profiler)                                                                                                                                                                                                                                                                                                                                                                                                               |
| DSC    | activation_checkpointing                  |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#activation-checkpointing)                                                                                                                                                                                                                                                                                                                                                                                                     |
| DSC    | sparse_attention                          |               | [Dictionary as described in Deepspeed documentation.](https://www.deepspeed.ai/docs/config-json/#sparse-attention)                                                                                                                                                                                                                                                                                                                                                                                                             |
| DSC    | zero_allow_untested_optimizer             |               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
