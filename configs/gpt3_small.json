{

  "tokenizer": {
      "type": "hf_gpt2tokenizerfast",
      "from_pretrained": true,
      "add_padding_token": false
  },

  "dataset": {
    "train_path": "path/to/tfrecords/train/*.tfrecords",
    "eval_path": "path/to/tfrecords/eval/*.tfrecords",
    "seed" : 1,
    "shuffle_input_filenames": true,
    "pretokenized": true,
    "filetype": "tfrecords",
    "mode": "chunks",
    "save_progress_every": 10000,
    "checkpoint_path": "gpt3_small_ckpt.txt",
    "resume_from_checkpoint": true
  },

  "train_steps": 572300,
  "batch_size": 256,
  "learning_rate": 0.0006,
  "validate_every": 100,
  "generate_every": 500,
  "generate_length": 256,
  "seq_len": 2048,
  "hidden_dim": 768,
  "n_layers": 12,
  "n_heads": 12,
  "dim_head": 64
}